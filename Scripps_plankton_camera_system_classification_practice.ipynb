{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification of Scripps plankton camera system images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this code as an introduction to using a convolutional neural network (CNN) to classify image data.  The data are images of different categories of plankton from the Scripps plankton camera system. \n",
    "\n",
    "\n",
    "1. New users are encouraged to COMMENT (NOT EDIT) the utility functions in CELL 2 as necessary to assist in interpretability for the user.  AGAIN, DO NOT CHANGE THE FUNCTIONS--just comment them if necessary.\n",
    "\n",
    "\n",
    "2. With the understanding gained from step 1, new users are expected to pre-process the data IN CELL 6. To do so, users must TYPE INTO CELL 6 the functions and their appropriate arguments FROM CELL 2 in the correct order to execute the functions and pre-process the data. \n",
    "\n",
    "\n",
    "3. Users should RUN CELL 7 to output some image data and provide some feedback that cell 6 was done correctly.\n",
    "\n",
    "\n",
    "4. Users are then expected to optimize the CNN by training it. Specifically, the goal is to get the loss (= training loss) and val_loss (= validation loss) as low as possible when training the CNN.  To optimize training, the hyperparameters IN CELL 4 need to be tweaked.  \n",
    "\n",
    "    New users should ONLY tweak the following parameters and ONLY use the following values in varying combinations:\n",
    "\n",
    "\n",
    "- `n_conv_layers` using the values 4,5,6,7,8,9,..., or 20. This value is the number of layers\n",
    "- `n_conv_filters` using ONLY the values 8, 16, 32, 64, or 128. This value is the number of filters\n",
    "- `kernel size` using ONLY the values (2,2) or (3,3) or (4,4). These values are the kernel sizes\n",
    "\n",
    "   Any and all combinations of those values should be tested to see which combination gives the lowest losses when executing the next step (step 5.)\n",
    "\n",
    "\n",
    "5. Using the combination of hyperparameters chosen in cell 4, users then have to train the CNN by RUNNING CELL 8.  Again, the goal is to get the loss and val_loss as low possible. Record somewhere (on a piece of paper, in a git comment, whatever) the hyperparameters for that run, the loss and val_loss at the end of training the CNN, and the the lowest loss/val_loss combination and epoch at which that occured out of all the epochs. \n",
    "\n",
    "    A few helpful notes: \n",
    "    - The goal is the get the loss to decrease and val_loss to decrease or at least not increase by tweaking hyperparameters\n",
    "    - Be sure to look at trends for > 20 epochs (i.e., not just the first couple of epochs)\n",
    "    - If the loss is decreasing and val_loss is increasing, that means the CNN is being overfit\n",
    "    - For each training run, record 1) the hyperparameters chosen, 2) the loss and val_loss at the end of training, and 3) the lowest loss and val_loss that occured during the entire training run and the epoch at which those lowest values occured.\n",
    "\n",
    "\n",
    "6. Repeat steps 4 and 5 as necessary to go through any and all combinations of hyperparameters, recording appropriate loss and val_loss and epoch information for each combination. Especially make note of the hyperparameters that give the lowest loss values, as those are the ones that will be used for prediction in the next step.\n",
    "\n",
    "\n",
    "7. Once the CNN has been trained to optimize the accuracy, the now trained CNN can be used to predict the labels for new image data.  To do so, fill in cell 9 and run it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing packages\n",
    "### Do NOT change anything in this cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "###### DO NOT CHANGE ANYTHING IN THIS CELL ###############\n",
    "##########################################################\n",
    "import glob\n",
    "import os\n",
    "import math\n",
    "from random import shuffle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Activation, Input, Dense, Conv2D, Dropout, Flatten, BatchNormalization\n",
    "from keras.regularizers import l1\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TerminateOnNaN, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# The code below are to allow tensorflow to work with Geforce RTX-2070 GPUs\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "# dynamically grow the memory used on the GPU\n",
    "config.gpu_options.allow_growth = True  \n",
    "# to log device placement (on which device the operation ran)                                  \n",
    "# (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "config.log_device_placement = True  \n",
    "sess = tf.Session(config=config)\n",
    "# set this TensorFlow session as the default session for Keras\n",
    "set_session(sess)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define utlity and training functions\n",
    "### Do NOT change anything in this cell , just comment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "######## COMMENT AS NEEDED, BUT DO NOT CHANGE THE FUNCTIONS ###########\n",
    "#######################################################################\n",
    "\n",
    "def get_image_files(root_dir, img_types):\n",
    "    full_paths = [x for x in os.walk(root_dir)] \n",
    "    imgs_temp = [os.path.join(ds,f) for ds,_,fs in full_paths for f in fs if f]   \n",
    "    imgs = [j for j in imgs_temp if any (k in j for k in img_types)]\n",
    "    return imgs\n",
    "\n",
    "def get_dimensions(files):\n",
    "    min_height, min_width = 10000, 10000\n",
    "    max_height, max_width = 0, 0\n",
    "    \n",
    "    for f in files:\n",
    "        img = cv.imread(f) \n",
    "        h,w = img.shape[:2] \n",
    "        \n",
    "        if h < min_height:\n",
    "            min_height = h \n",
    "        if h > max_height:\n",
    "            max_height = h\n",
    "        if w < min_width:\n",
    "            min_width = w\n",
    "        if w > max_width:\n",
    "            max_width = w\n",
    "            \n",
    "    return min_height, min_width, max_height, max_width\n",
    "\n",
    "def make_labels(files):\n",
    "    # Assume input is a list of complete file paths.\n",
    "    set_temp = {x.split('/')[-2] for x in files}\n",
    "    list_temp = list(set_temp) \n",
    "    list_new = sorted(list_temp) \n",
    "    label_dict = {list_new[x]:x for x in range(len(list_new))} \n",
    "    \n",
    "    return label_dict\n",
    "\n",
    "def make_train_val(files, lables):\n",
    "    train=[]\n",
    "    valid = []\n",
    "    train_prop = 0.6 \n",
    "    for key in labels: \n",
    "        temp = [f for f in files if key in f] #\n",
    "        train.extend(temp[:math.ceil(train_prop*len(temp))])\n",
    "        valid.extend(temp[math.ceil(train_prop*len(temp)):]) \n",
    "    return train, valid\n",
    "\n",
    "def get_batches(files, label_map, batch_size, resize_size, num_color_channels, augment=False, predict=False):\n",
    "    shuffle(files)\n",
    "    count = 0\n",
    "    num_files = len(files)\n",
    "    num_classes = len(label_map)\n",
    "\n",
    "    batch_out = np.zeros((batch_size, resize_size[0], resize_size[1], num_color_channels), dtype=np.uint8)\n",
    "    labels_out = np.zeros((batch_size,num_classes)) \n",
    "\n",
    "    while True: \n",
    "\n",
    "        f = files[count]\n",
    "        img = cv.imread(f)       \n",
    "\n",
    "        # Resize\n",
    "        rows,cols = img.shape[:2] \n",
    "        rc_ratio = rows/cols\n",
    "        if resize_size[0] > int(resize_size[1]*rc_ratio):\n",
    "            img = cv.resize(img, (resize_size[1], int(resize_size[1]*rc_ratio)))\n",
    "        else:\n",
    "            img = cv.resize(img, (int(resize_size[0]/rc_ratio), resize_size[0]))\n",
    "            \n",
    "        # Pad\n",
    "        rows,cols = img.shape[:2] \n",
    "        res = np.zeros((resize_size[0], resize_size[1], num_color_channels), dtype=np.uint8)\n",
    "        res[(resize_size[0]-rows)//2:(resize_size[0]-rows)//2+rows,\n",
    "            (resize_size[1]-cols)//2:(resize_size[1]-cols)//2+cols,:] = img \n",
    "                \n",
    "        # Augmentation \n",
    "        if augment:            \n",
    "            rows,cols = res.shape[:2]\n",
    "            M = cv.getRotationMatrix2D((cols/2,rows/2),np.random.uniform(0.0,360.0,1),1) \n",
    "            res = cv.warpAffine(res,M,(cols,rows))\n",
    "\n",
    "        # Change to gray scale if input argument num_color_channels = 1\n",
    "        if num_color_channels == 1: \n",
    "            res = cv.cvtColor(res, cv.COLOR_BGR2GRAY)\n",
    "            res = res[...,None] \n",
    "            \n",
    "        batch_out[count%batch_size,...] = res \n",
    "        \n",
    "        for k in label_map.keys():\n",
    "            if k in f: \n",
    "                labels_out[count%batch_size,:] = np_utils.to_categorical(label_map[k],num_classes) \n",
    "                break   \n",
    "                \n",
    "        count += 1\n",
    "        if count == num_files:\n",
    "            count = 0\n",
    "        if count%batch_size == 0: \n",
    "            if predict: \n",
    "                yield batch_out.astype(np.float)/255.\n",
    "            else: \n",
    "                yield batch_out.astype(np.float)/255., labels_out\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convolutional neural net classifier class\n",
    "### Do NOT change anything in this cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "###### DO NOT CHANGE ANYTHING IN THIS CELL ###############\n",
    "##########################################################\n",
    "\n",
    "# Convnet classifier\n",
    "class classifier():\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 n_classes,\n",
    "                 n_conv_layers=2,\n",
    "                 n_conv_filters=[32]*2, # individually customizable\n",
    "                 kernel_size=[(3,2)]*2, # list of integers or tuples\n",
    "                 n_dense_layers=1,\n",
    "                 dense_units=[32],\n",
    "                 dropout=[0.0]*3, # individually customizable\n",
    "                 strides=[(2,1)]*2,\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 l1_reg=0.0,\n",
    "                 lr=0.001\n",
    "                ):\n",
    "\n",
    "        if len(n_conv_filters) == 1:\n",
    "            n_conv_filters = n_conv_filters*n_conv_layers\n",
    "\n",
    "        if len(kernel_size) == 1:\n",
    "            kernel_size = kernel_size*n_conv_layers\n",
    "            \n",
    "        if len(dense_units) == 1:\n",
    "            dense_units = dense_units*n_dense_layers\n",
    "\n",
    "        if len(dropout) == 1:\n",
    "            dropout = dropout*(n_conv_layers+n_dense_layers)\n",
    "\n",
    "        if len(strides) == 1:\n",
    "            strides = strides*n_conv_layers\n",
    "\n",
    "        self.input_shape=input_shape\n",
    "        self.n_classes=n_classes\n",
    "        self.n_conv_layers=n_conv_layers\n",
    "        self.n_conv_filters=n_conv_filters\n",
    "        self.kernel_size=kernel_size\n",
    "        self.n_dense_layers=n_dense_layers\n",
    "        self.dense_units=dense_units\n",
    "        self.dropout=dropout\n",
    "        self.strides=strides\n",
    "        self.activation=activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.l1_reg=l1_reg\n",
    "        self.lr=lr\n",
    "        self.model = self.get_model()\n",
    "\n",
    "    def get_model(self):\n",
    "        I = Input(shape=self.input_shape, name='input')\n",
    "        X = I\n",
    "        # Add Conv layers\n",
    "        for i in range(self.n_conv_layers):\n",
    "            X = Conv2D(self.n_conv_filters[i], self.kernel_size[i], strides=self.strides[i], padding='same',\n",
    "                       data_format='channels_last', kernel_initializer=self.kernel_initializer,\n",
    "                       kernel_regularizer=l1(self.l1_reg), name='conv_{}'.format(i))(X)\n",
    "            X = Activation(self.activation)(X)\n",
    "#             X = BatchNormalization()(X)\n",
    "            X = Dropout(self.dropout[i])(X)\n",
    "        \n",
    "        X = Flatten()(X)\n",
    "        # Add Dense layers\n",
    "        for i in range(self.n_dense_layers):\n",
    "            X = Dense(self.dense_units[i], kernel_initializer=self.kernel_initializer,\n",
    "                      kernel_regularizer=l1(self.l1_reg), name='dense_{}'.format(i))(X)\n",
    "            X = Activation(self.activation)(X)\n",
    "#             X = BatchNormalization()(X)\n",
    "            X = Dropout(self.dropout[i+self.n_conv_layers])(X)\n",
    "        O = Dense(self.n_classes, activation='softmax', kernel_initializer=self.kernel_initializer,\n",
    "                  kernel_regularizer=l1(self.l1_reg), name='output')(X)\n",
    "        \n",
    "        model = Model(inputs=I, outputs=O)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.lr), metrics=['accuracy'])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training function\n",
    "### In this cell, tweak only the following parameters:\n",
    "- `n_conv_layers` using values 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, or 20\n",
    "- `n_conv_filters` using values 8, 16, 32, 64, or 128\n",
    "- `kernel_size` using values (2,2), (3,3), or (4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### CHANGE ONLY PARAMETERS INDICATED BELOW ###########\n",
    "def train(train_files, val_files, label_map, epochs=100, batch_size=8, common_size=(100,100), num_color_channels=3, \n",
    "          new_model=True, save_model_name='classification_model_1.hdf5'):\n",
    "    num_batches_per_epoch = len(train_files)//batch_size\n",
    "    \n",
    "    train_batch_generator = get_batches(train_files, label_map, batch_size, common_size, num_color_channels, augment=True)\n",
    "    val_batch_generator = get_batches(val_files, label_map, batch_size, common_size, num_color_channels)\n",
    "\n",
    "    checkpt = ModelCheckpoint(save_model_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    \n",
    "    if new_model: # create a new model\n",
    "######### CHANGE THIS SECTION TO CREATE NEW CONVOLUTIONAL ARCHITECTURE #################\n",
    "        model = classifier([common_size[0], common_size[1], num_color_channels],\n",
    "                           len(label_map),\n",
    "                           n_conv_layers=18,#number of convolutional layers\n",
    "                           n_conv_filters=[64],#number of filters for each conv. layer. Can just put one number to be repeated \n",
    "                           kernel_size=[(3,3)],#kernel size for each filter. Can just put one number to be repeated\n",
    "                           n_dense_layers=2,#number dense layers\n",
    "                           dense_units=[32],#number of nodes in dense layer. One number gets repeated\n",
    "                           dropout=[0.0],#proportion of nodes left out of each layer\n",
    "                           strides=([(1,1)]*2+[(2,2)])*6,#how filter moves across image. Can change stride for each filter. First number is left to right, second is up/down\n",
    "                           activation='relu',#activation function\n",
    "                           kernel_initializer='glorot_uniform',#kernel initializer\n",
    "                           l1_reg=0.0,#l1 norm regularizer\n",
    "                           lr=0.0001).model #lr = learning rate\n",
    "    else: # continue to train a previous model\n",
    "        print('Continuing training from a previous model')\n",
    "        model = load_model('models/'+save_model_name)\n",
    "\n",
    "    model.summary()\n",
    "    model.fit_generator(train_batch_generator, steps_per_epoch=num_batches_per_epoch, epochs=epochs,\n",
    "                        verbose=1, callbacks=[checkpt, TerminateOnNaN()], \n",
    "                        validation_data=val_batch_generator, validation_steps=len(val_files)//batch_size)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prediction function\n",
    "### Do NOT change anything in this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "###### DO NOT CHANGE ANYTHING IN THIS CELL ###############\n",
    "##########################################################\n",
    "\n",
    "def predict(files, label_map, common_size=(100,100), num_color_channels=3, saved_model_name='classification_model_1.hdf5'):\n",
    "    model = load_model(saved_model_name)\n",
    "    num_batches_per_epoch = len(files)    \n",
    "    predict_batch_generator = get_batches(files, {}, batch_size, common_size, num_color_channels)\n",
    "\n",
    "    predicts = []\n",
    "    p = model.predict_generator(predict_batch_generator, steps_per_epoch=num_batches_per_epoch)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data preprocessing\n",
    "#### Fill in the appropriate values and use the appropriate utility functions from cell 2 to pre-process the data \n",
    "#### Directions and information are in the comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096\n",
      "['/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images/Centric_diatom/SPCP2-1429588285-167161-000-1784-1076-144-136.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images/Centric_diatom/SPCP2-1432008290-105571-001-1128-408-216-248.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images/Barnacle_nauplii/SPC2-1426227077-725186-000-0-1996-224-120.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images/Barnacle_nauplii/SPC2-1431461547-035631-004-1772-456-136-80.jpg']\n",
      "Over all images - minimum height: 32, minimum width: 24, maximum height: 880, maximum width:920\n",
      "{'Ascidian_larvae': 0, 'Bacteriastrum': 1, 'Barnacle_cypris': 2, 'Barnacle_nauplii': 3, 'Centric_diatom': 4, 'Ceratium': 5, 'Ceratium_falcatiforme': 6, 'Ceratiusm_two_cells': 7, 'Cheatoceros': 8, 'Ciliate': 9, 'Dinoflagellate_generic': 10, 'Dinophysis_caudata': 11, 'Dinophysis_fortii': 12, 'Gonyaulax_spinifera': 13, 'Prorocentrum': 14, 'Prorocentrum_micans': 15, 'Protoperidinium': 16, 'Pyrocystis_lunula': 17}\n",
      "2084\n",
      "1401\n"
     ]
    }
   ],
   "source": [
    "## TO DO: Get full paths to all image classification data and save it in the vairable root_dir\n",
    "# Data is assumed to reside under the directory \"root_dir\", and data for each class is assumed to reside in a separate subfolder\n",
    "# The commmented out line below is an example and won't work on most computers\n",
    "#root_dir = '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_images'\n",
    "\n",
    "\n",
    "## TO DO: Look in the directory of images and determine if any needed extensions need to be added to the list below\n",
    "img_types=['.jpg', '.tiff', '.tif', '.png', '.jpeg'] \n",
    "\n",
    "# #TO DO: Get the image files and save them in the variable files\n",
    "\n",
    "\n",
    "\n",
    "print(len(files))\n",
    "print(files[0:4])\n",
    "\n",
    "## TO DO: Get the dimension range of the data for informational purposes\n",
    "# Put min height in variable minh, min width in variable minw, max heigh in variable max h, max width in maxw\n",
    "\n",
    "\n",
    "\n",
    "print('Over all images - minimum height: {}, minimum width: {}, maximum height: {}, maximum width:{}'.format(minh,minw,maxh,maxw))\n",
    "\n",
    "## TO DO: Assign numerical labels to categories - the number of categories is equal to the number of subfolders\n",
    "# Put dictionary of categories and numerical labels in the variable label_map\n",
    "\n",
    "\n",
    "\n",
    "print(label_map)\n",
    "\n",
    "## TO DO: Split the data into training and validation\n",
    "# Save training data set in variable train_files and validation data set in val_files\n",
    "\n",
    "\n",
    "\n",
    "print(len(train_files))\n",
    "print(len(val_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get batches and test functions run above\n",
    "#### Run the cell below to get some example data to see if images are processed properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO: Run this cell to get batches and process some example data\n",
    "batch_size = 32\n",
    "common_size = (100,100)\n",
    "num_color_channels = 3\n",
    "train_files = train_files[:len(train_files)//batch_size*batch_size]\n",
    "g = get_batches(train_files, label_map, batch_size, common_size, num_color_channels, augment=True)\n",
    "b,l = next(g)\n",
    "for i in b:\n",
    "    plt.figure()\n",
    "    plt.imshow(i[...,::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Training classifier\n",
    "#### Run the training classifer cell to train the CNN classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "# Note: all images are resized to common_size.  \n",
    "# Images smaller than common_size will be enlarged using interpolation.  Images larger will be shrunk using decimation.\n",
    "batch_size = 32\n",
    "epochs = 2000\n",
    "train_files = train_files[:len(train_files)//batch_size*batch_size]\n",
    "val_files = val_files[:len(val_files)//batch_size*batch_size]\n",
    "print(len(train_files))\n",
    "print(len(val_files))\n",
    "model = train(train_files, val_files, label_map, epochs=epochs, batch_size=batch_size, common_size=(200,200), num_color_channels=3, \n",
    "              new_model=True, save_model_name='classification_model_1.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Prediction on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
