{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning convolutional neural networks\n",
    "Use this notebook to fine-tune pre-trained networks from Keras found here https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Import packages in cell 1.\n",
    "2. Comment with enough detail to understand what the utility functions do in cell 2.\n",
    "3. Pre-process data, completing code with TO DO statements above them in cell 3\n",
    "4. Build, compile, and train model, completing code with TO DO statements in cell 4 (4a and 4b)\n",
    "5. Predict how well model did, completing code with TO DO statements in cell 5\n",
    "6. Use this notebook as a template to fine tune a different pre-trained model architecture (found at https://keras.io/applications/), making adjustments for that model as necessary\n",
    "7. Compare performance for at least 3 model architectures and document which is the best to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "from random import shuffle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Imports for deep learning specifically\n",
    "from keras.applications.inception_v3 import InceptionV3#--[don't need if running Xception]\n",
    "from keras.applications.xception import Xception\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,  Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining utility functions\n",
    "Do NOT change the functions in this cell, ONLY comment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_files(root_dir, img_types):\n",
    "    #os.walk creates 3-tuple with (dirpath, dirnames, filenames)\n",
    "    \n",
    "    # Get all the root directories, subdirectories, and files\n",
    "    full_paths = [x for x in os.walk(root_dir)] \n",
    "    imgs_temp = [os.path.join(ds,f) for ds,_,fs in full_paths for f in fs if f]   \n",
    "    \n",
    "    # Filter out so only have directories with .jpg, .tiff, .tif, .png, .jpeg\n",
    "    imgs = [j for j in imgs_temp if any (k in j for k in img_types)]\n",
    "    return imgs\n",
    "\n",
    "def get_dimensions(files):\n",
    "    # Set starting points for min and max dimensions\n",
    "    min_height, min_width = 10000, 10000\n",
    "    max_height, max_width = 0, 0\n",
    "    \n",
    "    for f in files:\n",
    "        # Read in images\n",
    "        img = cv.imread(f) # Read in images\n",
    "        h,w = img.shape[:2] # get height and width\n",
    "        \n",
    "        # Update min and max values, if necessary\n",
    "        if h < min_height:\n",
    "            min_height = h \n",
    "        if h > max_height:\n",
    "            max_height = h\n",
    "        if w < min_width:\n",
    "            min_width = w\n",
    "        if w > max_width:\n",
    "            max_width = w\n",
    "            \n",
    "    return min_height, min_width, max_height, max_width\n",
    "\n",
    "def make_labels(files):\n",
    "    # Assume input is a list of complete file paths.\n",
    "    # Count the number of unique directory names that are immediate parent of the files.\n",
    "    # Order the directory names alphabetically from a-z, and associate labels accordingly.\n",
    "    set_temp = {x.split('/')[-2] for x in files} #doing as set to get only unique values\n",
    "    list_temp = list(set_temp) #Change to list so can interate over it\n",
    "    list_new = sorted(list_temp) #Alphabetizing\n",
    "    label_dict = {list_new[x]:x for x in range(len(list_new))} #create dictionary with category:index\n",
    "    \n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def make_train_val_test(files, labels):\n",
    "    train=[]\n",
    "    valid = []\n",
    "    test =[]\n",
    "    train_labels_name = []\n",
    "    valid_labels_name = []\n",
    "    test_labels_name = []\n",
    "    train_prop = 0.6 #proportion of data set that will be training\n",
    "    val_prop = 0.2 #proprotion of dataset that is validation\n",
    "    for key in labels: #going through each key\n",
    "        temp = [f for f in files if key in f] #getting all files in a specific category (ie key)\n",
    "        lower_prop = math.ceil(train_prop*len(temp))\n",
    "        train.extend(temp[:lower_prop]) #training data set\n",
    "        valid.extend(temp[lower_prop:lower_prop+math.ceil(val_prop*len(temp))]) # validation data set\n",
    "        test.extend(temp[lower_prop+math.ceil(val_prop*len(temp)):])\n",
    "    train_labels_name = [x.split('/')[-2] for x in train]\n",
    "    valid_labels_name = [x.split('/')[-2] for x in valid]\n",
    "    test_labels_name =  [x.split('/')[-2] for x in test]\n",
    "    return train, valid, test, train_labels_name, valid_labels_name, test_labels_name\n",
    "\n",
    "\n",
    "def get_batches(files, label_map, batch_size, resize_size, num_color_channels, augment=False, predict=False, do_shuffle = True):\n",
    "    if do_shuffle:\n",
    "        shuffle(files)\n",
    "    count = 0\n",
    "    num_files = len(files)\n",
    "    num_classes = len(label_map)\n",
    "    \n",
    "    batch_out = np.zeros((batch_size, resize_size[0], resize_size[1], num_color_channels), dtype=np.uint8)\n",
    "    labels_out = np.zeros((batch_size,num_classes)) #one-hot labeling, which is why have num_classes num of col.   \n",
    "\n",
    "    while True: # while True is to ensure when yielding that start here and not previous lines\n",
    "\n",
    "        f = files[count]\n",
    "        img = cv.imread(f)       \n",
    "\n",
    "        # Resize\n",
    "        # First resize while keeping aspect ratio\n",
    "        rows,cols = img.shape[:2] # Define in input num_color_channels in case want black and white\n",
    "        rc_ratio = rows/cols\n",
    "        if resize_size[0] > int(resize_size[1]*rc_ratio):# if resize rows > rows with given aspect ratio\n",
    "            img = cv.resize(img, (resize_size[1], int(resize_size[1]*rc_ratio)))#NB: resize dim arg are col,row\n",
    "        else:\n",
    "            img = cv.resize(img, (int(resize_size[0]/rc_ratio), resize_size[0]))\n",
    "            \n",
    "        # Second, pad to final size\n",
    "        rows,cols = img.shape[:2] #find new num rows and col of resized image\n",
    "        res = np.zeros((resize_size[0], resize_size[1], num_color_channels), dtype=np.uint8)#array of zeros\n",
    "        res[(resize_size[0]-rows)//2:(resize_size[0]-rows)//2+rows,\n",
    "            (resize_size[1]-cols)//2:(resize_size[1]-cols)//2+cols,:] = img # fill in image in middle of zeros\n",
    "                \n",
    "        # Augmentation \n",
    "        if augment:            \n",
    "            rows,cols = res.shape[:2]\n",
    "            # calculates affine rotation with random angle rotation, keeping same center and scale\n",
    "            M = cv.getRotationMatrix2D((cols/2,rows/2),np.random.uniform(0.0,360.0,1),1) \n",
    "            # applies affine rotation\n",
    "            res = cv.warpAffine(res,M,(cols,rows))\n",
    "\n",
    "        # Change to gray scale if input argument num_color_channels = 1\n",
    "        if num_color_channels == 1: \n",
    "            res = cv.cvtColor(res, cv.COLOR_BGR2GRAY)# convert from bgr to gray\n",
    "            res = res[...,None] # add extra dimension with blank values to very end, needed for keras\n",
    "            \n",
    "        batch_out[count%batch_size,...] = res # put image in position in batch, never to exceed size of batch\n",
    "        \n",
    "        for k in label_map.keys():\n",
    "            if k in f: #if a category name is found in the path to the file of the image\n",
    "                labels_out[count%batch_size,:] = np_utils.to_categorical(label_map[k],num_classes) #one hot labeling\n",
    "                break   \n",
    "                \n",
    "        count += 1\n",
    "        if count == num_files:# if gone through all files, restart the counter\n",
    "            count = 0\n",
    "        if count%batch_size == 0: #if gone through enough files to make a full batch\n",
    "            if predict: # i.e., there is no label for this batch of images, so in prediction mode\n",
    "                yield batch_out.astype(np.float)/255.\n",
    "            else: # training\n",
    "                yield batch_out.astype(np.float)/255., labels_out\n",
    "            \n",
    "            \n",
    "            \n",
    "def convert_to_class(prediction,label_map):\n",
    "    predict_max = np.argmax(prediction,axis=1)#provides index of max value out of prediction classes\n",
    "    predict_label = []\n",
    "    for i in range(len(predict_max)):\n",
    "        for k,v in label_map.items():\n",
    "                if predict_max[i] == v:\n",
    "                    predict_label.append(k)\n",
    "    return predict_label    \n",
    "\n",
    "def prop_correct(predict_label,actual_label):\n",
    "    correct_class = []\n",
    "    for i in range(len(predict_label)):\n",
    "        if predict_label[i]==actual_label[i]:\n",
    "            correct_class.append(1)\n",
    "        else:\n",
    "            correct_class.append(0)\n",
    "    num_correct = sum(correct_class)\n",
    "    proportion_correct = num_correct/len(predict_label)\n",
    "    return proportion_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files is  1679\n",
      "example file names are  ['/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other/Ciliate/SPCP2-1551916275-018743-003-12-304-120-104.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other/Ciliate/SPCP2-1549460119-039576-001-664-1800-96-104.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other/Ciliate/SPCP2-1564409932-302705-002-192-932-104-96.jpg', '/Users/dtaniguchi/Research/Image_classification/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other/Ciliate/SPCP2-1564466851-138834-002-792-708-136-144.jpg']\n",
      "Over all images - minimum height: 24, minimum width: 32, maximum height: 312, maximum width:448\n",
      "{'Ciliate': 0, 'Other': 1}\n",
      "1008\n",
      "336\n",
      "335\n",
      "train labels length is  1008\n",
      "validation labels length is 336\n",
      "test labels length is 335\n"
     ]
    }
   ],
   "source": [
    "# Get full paths to all classification data\n",
    "# Data is assumed to reside under the directory \"root_dir\", and data for each class is assumed to reside in a separate subfolder\n",
    "\n",
    "# TO DO: define in the variable root_dir the directory path to where the folders with the images are located\n",
    "root_dir = \n",
    "\n",
    "\n",
    "# TO DO: add in any additional image types in path above that are not already listed in the img_types variable below\n",
    "img_types=['.jpg', '.tiff', '.tif', '.png', '.jpeg']\n",
    "\n",
    "files = get_image_files(root_dir, img_types)\n",
    "print('number of files is ',len(files))\n",
    "print('example file names are ', files[0:4])\n",
    "\n",
    "# Get the dimension range of the data for informational purposes\n",
    "minh,minw,maxh,maxw = get_dimensions(files)\n",
    "print('Over all images - minimum height: {}, minimum width: {}, maximum height: {}, maximum width:{}'.format(minh,minw,maxh,maxw))\n",
    "\n",
    "# Assign numerical labels to categories - the number of categories is equal to the number of subfolders\n",
    "label_map = make_labels(files)\n",
    "print(label_map)\n",
    "\n",
    "# TO DO: Using the appropriate utility function from cell 2, divide data into training, validation, and testing data\n",
    "# Variable names should be as follows:\n",
    "# train_files = training data\n",
    "# val_files = validation data\n",
    "# test_files = testing data\n",
    "# train_labels_name = training labels\n",
    "# val_labels_name = validation labels\n",
    "# test_labels_name = testing data labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Print length of each data set and labels array\n",
    "print('length of trainig data is ',len(train_files))\n",
    "print('length of trainig data is ',len(val_files))\n",
    "print('length of trainig data is ',len(test_files))\n",
    "\n",
    "print('train labels length is ',len(train_labels_name))\n",
    "print('validation labels length is', len(val_labels_name))\n",
    "print('test labels length is', len(test_labels_name))    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine tuning\n",
    "The code below was modified from https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes and must be adapted for use with xception instead of InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Creating base pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base pre-trained model\n",
    "# TO DO: Think about if you should include the top layers (the layers used for classification in the ORIGINAL model). \n",
    "# If you should, type in include_top = True, otherwise, use include_top = False\n",
    "base_model = InceptionV3(weights='imagenet', include_top= ) \n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer\n",
    "x = Dense(4, activation='relu')(x)\n",
    "\n",
    "# Add logistic layer -- let's say we have x classes--determined by len(label_map)\n",
    "predictions = Dense(len(label_map), activation='softmax')(x)\n",
    "\n",
    "# Below is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model (should be done AFTER setting layers to non-trainable)\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics= ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Training existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1000\n",
      "31/31 [==============================] - 48s 2s/step - loss: 0.6802 - acc: 0.5857 - val_loss: 0.6890 - val_acc: 0.5469\n",
      "Epoch 2/1000\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.5830 - acc: 0.7209 - val_loss: 0.6468 - val_acc: 0.6463\n",
      "Epoch 3/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.5293 - acc: 0.7510 - val_loss: 0.5980 - val_acc: 0.7121\n",
      "Epoch 4/1000\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.4926 - acc: 0.7862 - val_loss: 0.5839 - val_acc: 0.7293\n",
      "Epoch 5/1000\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.4724 - acc: 0.7964 - val_loss: 0.5285 - val_acc: 0.7903\n",
      "Epoch 6/1000\n",
      "31/31 [==============================] - 41s 1s/step - loss: 0.4820 - acc: 0.7712 - val_loss: 0.6122 - val_acc: 0.6714\n",
      "Epoch 7/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.4384 - acc: 0.7964 - val_loss: 0.5577 - val_acc: 0.7214\n",
      "Epoch 8/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.4418 - acc: 0.7993 - val_loss: 0.5245 - val_acc: 0.7668\n",
      "Epoch 9/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.4420 - acc: 0.8075 - val_loss: 0.5516 - val_acc: 0.7449\n",
      "Epoch 10/1000\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.4358 - acc: 0.8044 - val_loss: 0.5747 - val_acc: 0.7183\n",
      "Epoch 11/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.4444 - acc: 0.7983 - val_loss: 0.5553 - val_acc: 0.7480\n",
      "Epoch 12/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4164 - acc: 0.8185 - val_loss: 0.5327 - val_acc: 0.7480\n",
      "Epoch 13/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.4157 - acc: 0.8124 - val_loss: 0.5371 - val_acc: 0.7574\n",
      "Epoch 14/1000\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.3877 - acc: 0.8347 - val_loss: 0.5881 - val_acc: 0.7167\n",
      "Epoch 15/1000\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.4009 - acc: 0.8327 - val_loss: 0.5904 - val_acc: 0.7074\n",
      "Epoch 16/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.4139 - acc: 0.8176 - val_loss: 0.5107 - val_acc: 0.7903\n",
      "Epoch 17/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.4201 - acc: 0.8105 - val_loss: 0.5114 - val_acc: 0.7653\n",
      "Epoch 18/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.3865 - acc: 0.8396 - val_loss: 0.5440 - val_acc: 0.7371\n",
      "Epoch 19/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4104 - acc: 0.8327 - val_loss: 0.5246 - val_acc: 0.7371\n",
      "Epoch 20/1000\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.4526 - acc: 0.7913 - val_loss: 0.6258 - val_acc: 0.6870\n",
      "Epoch 21/1000\n",
      "31/31 [==============================] - 42s 1s/step - loss: 0.3895 - acc: 0.8125 - val_loss: 0.5755 - val_acc: 0.7293\n",
      "Epoch 22/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3929 - acc: 0.8185 - val_loss: 0.5604 - val_acc: 0.7312\n",
      "Epoch 23/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.3922 - acc: 0.8286 - val_loss: 0.5600 - val_acc: 0.7214\n",
      "Epoch 24/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.3899 - acc: 0.8358 - val_loss: 0.5207 - val_acc: 0.7480\n",
      "Epoch 25/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.3987 - acc: 0.8287 - val_loss: 0.5169 - val_acc: 0.7621\n",
      "Epoch 26/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3921 - acc: 0.8377 - val_loss: 0.5429 - val_acc: 0.7543\n",
      "Epoch 27/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4017 - acc: 0.8246 - val_loss: 0.5683 - val_acc: 0.7308\n",
      "Epoch 28/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.3870 - acc: 0.8126 - val_loss: 0.6253 - val_acc: 0.7136\n",
      "Epoch 29/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4174 - acc: 0.8276 - val_loss: 0.5719 - val_acc: 0.7340\n",
      "Epoch 30/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3945 - acc: 0.8307 - val_loss: 0.5772 - val_acc: 0.7152\n",
      "Epoch 31/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3929 - acc: 0.8276 - val_loss: 0.5781 - val_acc: 0.7058\n",
      "Epoch 32/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3962 - acc: 0.8267 - val_loss: 0.6154 - val_acc: 0.7230\n",
      "Epoch 33/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4161 - acc: 0.8135 - val_loss: 0.6469 - val_acc: 0.7136\n",
      "Epoch 34/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.3914 - acc: 0.8297 - val_loss: 0.5240 - val_acc: 0.7684\n",
      "Epoch 35/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4203 - acc: 0.8196 - val_loss: 0.6283 - val_acc: 0.6933\n",
      "Epoch 36/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4007 - acc: 0.8106 - val_loss: 0.5088 - val_acc: 0.7793\n",
      "Epoch 37/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4054 - acc: 0.8145 - val_loss: 0.5711 - val_acc: 0.7121\n",
      "Epoch 38/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3971 - acc: 0.8186 - val_loss: 0.6160 - val_acc: 0.7199\n",
      "Epoch 39/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.3748 - acc: 0.8296 - val_loss: 0.4885 - val_acc: 0.7621\n",
      "Epoch 40/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.4238 - acc: 0.8165 - val_loss: 0.5497 - val_acc: 0.7465\n",
      "Epoch 41/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.3726 - acc: 0.8397 - val_loss: 0.5868 - val_acc: 0.7183\n",
      "Epoch 42/1000\n",
      "31/31 [==============================] - 46s 1s/step - loss: 0.4082 - acc: 0.8095 - val_loss: 0.5211 - val_acc: 0.7762\n",
      "Epoch 43/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3882 - acc: 0.8205 - val_loss: 0.6088 - val_acc: 0.7234\n",
      "Epoch 44/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.4007 - acc: 0.8357 - val_loss: 0.5315 - val_acc: 0.7606\n",
      "Epoch 45/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.3843 - acc: 0.8357 - val_loss: 0.5819 - val_acc: 0.7261\n",
      "Epoch 46/1000\n",
      "31/31 [==============================] - 43s 1s/step - loss: 0.4096 - acc: 0.8105 - val_loss: 0.4958 - val_acc: 0.7715\n",
      "Epoch 47/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.4030 - acc: 0.8286 - val_loss: 0.5097 - val_acc: 0.7684\n",
      "Epoch 48/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3647 - acc: 0.8417 - val_loss: 0.5540 - val_acc: 0.7136\n",
      "Epoch 49/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.4115 - acc: 0.8155 - val_loss: 0.5412 - val_acc: 0.7387\n",
      "Epoch 50/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.3961 - acc: 0.8195 - val_loss: 0.5127 - val_acc: 0.7465\n",
      "Epoch 51/1000\n",
      "31/31 [==============================] - 44s 1s/step - loss: 0.3992 - acc: 0.8377 - val_loss: 0.5000 - val_acc: 0.7825\n",
      "Epoch 52/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.3513 - acc: 0.8498 - val_loss: 0.5059 - val_acc: 0.7621\n",
      "Epoch 53/1000\n",
      "31/31 [==============================] - 45s 1s/step - loss: 0.4040 - acc: 0.8226 - val_loss: 0.5187 - val_acc: 0.7449\n",
      "Epoch 54/1000\n",
      "12/31 [==========>...................] - ETA: 15s - loss: 0.3551 - acc: 0.8464"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8fa49fdb57ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                    \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_labels_oh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                    \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mBS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                    callbacks=[ES])\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## TO DO: try the following batch sizes, one at a time: 16, 32, 64, recording accuracy for all\n",
    "BS = 32\n",
    "\n",
    "# Initializing other parameters\n",
    "EPOCHS = 1000\n",
    "im_wid = 150\n",
    "im_height = 150 \n",
    "\n",
    "# Construct the training image generator for data augmentation\n",
    "data_gen = ImageDataGenerator(featurewise_center = False, samplewise_center = False,\n",
    "                             featurewise_std_normalization = False, samplewise_std_normalization=False,\n",
    "                             rotation_range = 360, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                             zoom_range = 0.5, fill_mode = 'constant',cval=0,horizontal_flip = True,\n",
    "                             vertical_flip = True, rescale = None)\n",
    "\n",
    "# Get array of training and validaiton images \n",
    "train_gen = get_batches(train_files, label_map, batch_size = len(train_files),resize_size=[im_height,im_wid],\n",
    "                       num_color_channels=3)\n",
    "val_gen = get_batches(val_files,label_map,batch_size = len(val_files),resize_size=[im_height,im_wid],\n",
    "                     num_color_channels = 3)\n",
    "\n",
    "\n",
    "train_data, train_labels_oh = next(train_gen) #one-hot encoded data\n",
    "val_data, val_labels_oh = next(val_gen)\n",
    "\n",
    "# Train the network\n",
    "ES = EarlyStopping(monitor='val_loss',patience=20,verbose=0)# callback to stop if validation loss has not improved in 20 iterations\n",
    "model.fit_generator(data_gen.flow(train_data,train_labels_oh, batch_size = BS),\n",
    "                   steps_per_epoch=len(train_files)//BS,epochs = EPOCHS,\n",
    "                   validation_data=data_gen.flow(val_data,val_labels_oh,batch_size=BS),\n",
    "                   validation_steps = len(val_files)//BS,\n",
    "                   callbacks=[ES])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using test data\n",
    "predict_gen = get_batches(test_files,label_map,batch_size=1,resize_size=[im_height,im_wid],\n",
    "                          num_color_channels=3, predict = True,do_shuffle=False)\n",
    "prediction = model.predict_generator(predict_gen,steps = len(test_files))\n",
    "\n",
    "# TO DO: Use the appropriate utility function from cell 2 to convert predictions (saved in the variable prediction\n",
    "    # to a classification category\n",
    "    # Save the output in the variable predict_class\n",
    "\n",
    "    \n",
    "\n",
    "# TO DO: Determine the proportion of classifications that were classified correctly using the appropriate utility\n",
    "    # function from cell 2. Save that proportion in the variable proportion_correct\n",
    "    \n",
    "    \n",
    "# Printing proportion correct    \n",
    "print(proportion_correct)\n",
    "\n",
    "# TO DO: record the variable settings (e.g., CNN architecture, batch size, epochs, optimizers, and proportion correct) \n",
    "    # in some document (e.g., excel spreadsheet, git, etc.)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model and compiling first layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = Xception(include_top=False, weights='imagenet', input_tensor=None, input_shape=(880,920,3), pooling=None)\n",
    "base_modelX = Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "# Adding global spatial average pooling layer\n",
    "xX = base_modelX.output\n",
    "xX = GlobalAveragePooling2D()(xX)\n",
    "\n",
    "# Adding in fully-connected layer\n",
    "xX = Dense(4, activation='relu')(xX)\n",
    "\n",
    "# Logistic layer for number of classes\n",
    "## [Add in flexible number of classes]\n",
    "predictions = Dense(len(label_map),activation='softmax')(xX)\n",
    "\n",
    "# Model that will be trained\n",
    "modelX = Model(inputs = base_modelX.input, outputs = predictions)\n",
    "\n",
    "# Training only top layers\n",
    "for layer in base_modelX.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Compile model\n",
    "## [Choose different optimizers]\n",
    "modelX.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [Vary batch size, maybe epochs]\n",
    "EPOCHS = 1000\n",
    "BS = 32\n",
    "\n",
    "im_height = 150\n",
    "im_wid = 150\n",
    "\n",
    "# Creating training image generator for data augmentation\n",
    "data_gen = ImageDataGenerator(featurewise_center = False, samplewise_center = False, \n",
    "                             featurewise_std_normalization = False, samplewise_std_normalization = False,\n",
    "                             rotation_range = 360, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                             zoom_range = 0.5, fill_mode = 'constant',cval = 0, horizontal_flip = True,\n",
    "                             vertical_flip = True, rescale = None)\n",
    "\n",
    "# Getting training and validation generators\n",
    "train_gen = get_batches(train_files, label_map, batch_size = len(train_files),\n",
    "                       resize_size = [im_height,im_wid],num_color_channels=3)\n",
    "val_gen = get_batches(val_files, label_map,batch_size = len(val_files),\n",
    "                     resize_size = [im_height,im_wid],num_color_channels=3)\n",
    "train_data,train_labels_oh = next(batch_gen)\n",
    "val_data,val_labels_oh = next(val_gen)\n",
    "\n",
    "# Training network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
