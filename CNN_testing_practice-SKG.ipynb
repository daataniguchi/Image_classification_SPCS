{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning convolutional neural networks\n",
    "Use this notebook to fine-tune pre-trained networks from Keras found here https://keras.io/applications/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. Import packages in cell 1.\n",
    "2. Comment with enough detail to understand what the utility functions do in cell 2.\n",
    "3. Pre-process data, completing code with TO DO statements above them in cell 3\n",
    "4. Build, compile, and train model, completing code with TO DO statements in cell 4 (4a and 4b)\n",
    "5. Predict how well model did, completing code with TO DO statements in cell 5\n",
    "6. Use this notebook as a template to fine tune a different pre-trained model architecture (found at https://keras.io/applications/), making adjustments for that model as necessary\n",
    "7. Compare performance for at least 3 model architectures and document which is the best to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from random import shuffle\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Imports for deep learning specifically\n",
    "from keras.applications.inception_v3 import InceptionV3#--[don't need if running Xception]\n",
    "from keras.applications.xception import Xception\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D,  Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining utility functions\n",
    "Do NOT change the functions in this cell, ONLY comment as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_files(root_dir, img_types):\n",
    "    #os.walk creates 3-tuple with (dirpath, dirnames, filenames)\n",
    "    \n",
    "    # Get all the root directories, subdirectories, and files\n",
    "    full_paths = [x for x in os.walk(root_dir)] \n",
    "    imgs_temp = [os.path.join(ds,f) for ds,_,fs in full_paths for f in fs if f]   \n",
    "    \n",
    "    # Filter out so only have directories with .jpg, .tiff, .tif, .png, .jpeg\n",
    "    imgs = [j for j in imgs_temp if any (k in j for k in img_types)]\n",
    "    return imgs\n",
    "\n",
    "def get_dimensions(files):\n",
    "    # Set starting points for min and max dimensions\n",
    "    min_height, min_width = 10000, 10000\n",
    "    max_height, max_width = 0, 0\n",
    "    \n",
    "    for f in files:\n",
    "        # Read in images\n",
    "        img = cv.imread(f) # Read in images\n",
    "        h,w = img.shape[:2] # get height and width\n",
    "        \n",
    "        # Update min and max values, if necessary\n",
    "        if h < min_height:\n",
    "            min_height = h \n",
    "        if h > max_height:\n",
    "            max_height = h\n",
    "        if w < min_width:\n",
    "            min_width = w\n",
    "        if w > max_width:\n",
    "            max_width = w\n",
    "            \n",
    "    return min_height, min_width, max_height, max_width\n",
    "\n",
    "def make_labels(files):\n",
    "    # Assume input is a list of complete file paths.\n",
    "    # Count the number of unique directory names that are immediate parent of the files.\n",
    "    # Order the directory names alphabetically from a-z, and associate labels accordingly.\n",
    "    set_temp = {x.split('/')[-2] for x in files} #doing as set to get only unique values\n",
    "    list_temp = list(set_temp) #Change to list so can interate over it\n",
    "    list_new = sorted(list_temp) #Alphabetizing\n",
    "    label_dict = {list_new[x]:x for x in range(len(list_new))} #create dictionary with category:index\n",
    "    \n",
    "    return label_dict\n",
    "\n",
    "\n",
    "def make_train_val_test(files, labels):\n",
    "    train=[]\n",
    "    valid = []\n",
    "    test =[]\n",
    "    train_labels_name = []\n",
    "    valid_labels_name = []\n",
    "    test_labels_name = []\n",
    "    train_prop = 0.6 #proportion of data set that will be training\n",
    "    val_prop = 0.2 #proprotion of dataset that is validation\n",
    "    for key in labels: #going through each key\n",
    "        temp = [f for f in files if key in f] #getting all files in a specific category (ie key)\n",
    "        lower_prop = math.ceil(train_prop*len(temp))\n",
    "        train.extend(temp[:lower_prop]) #training data set\n",
    "        valid.extend(temp[lower_prop:lower_prop+math.ceil(val_prop*len(temp))]) # validation data set\n",
    "        test.extend(temp[lower_prop+math.ceil(val_prop*len(temp)):])\n",
    "    train_labels_name = [x.split('/')[-2] for x in train]\n",
    "    valid_labels_name = [x.split('/')[-2] for x in valid]\n",
    "    test_labels_name =  [x.split('/')[-2] for x in test]\n",
    "    return train, valid, test, train_labels_name, valid_labels_name, test_labels_name\n",
    "\n",
    "\n",
    "def get_batches(files, label_map, batch_size, resize_size, num_color_channels, augment=False, predict=False, do_shuffle = True):\n",
    "    if do_shuffle:\n",
    "        shuffle(files)\n",
    "    count = 0\n",
    "    num_files = len(files)\n",
    "    num_classes = len(label_map)\n",
    "    \n",
    "    batch_out = np.zeros((batch_size, resize_size[0], resize_size[1], num_color_channels), dtype=np.uint8)\n",
    "    labels_out = np.zeros((batch_size,num_classes)) #one-hot labeling, which is why have num_classes num of col.   \n",
    "\n",
    "    while True: # while True is to ensure when yielding that start here and not previous lines\n",
    "\n",
    "        f = files[count]\n",
    "        img = cv.imread(f)       \n",
    "\n",
    "        # Resize\n",
    "        # First resize while keeping aspect ratio\n",
    "        rows,cols = img.shape[:2] # Define in input num_color_channels in case want black and white\n",
    "        rc_ratio = rows/cols\n",
    "        if resize_size[0] > int(resize_size[1]*rc_ratio):# if resize rows > rows with given aspect ratio\n",
    "            img = cv.resize(img, (resize_size[1], int(resize_size[1]*rc_ratio)))#NB: resize dim arg are col,row\n",
    "        else:\n",
    "            img = cv.resize(img, (int(resize_size[0]/rc_ratio), resize_size[0]))\n",
    "            \n",
    "        # Second, pad to final size\n",
    "        rows,cols = img.shape[:2] #find new num rows and col of resized image\n",
    "        res = np.zeros((resize_size[0], resize_size[1], num_color_channels), dtype=np.uint8)#array of zeros\n",
    "        res[(resize_size[0]-rows)//2:(resize_size[0]-rows)//2+rows,\n",
    "            (resize_size[1]-cols)//2:(resize_size[1]-cols)//2+cols,:] = img # fill in image in middle of zeros\n",
    "                \n",
    "        # Augmentation \n",
    "        if augment:            \n",
    "            rows,cols = res.shape[:2]\n",
    "            # calculates affine rotation with random angle rotation, keeping same center and scale\n",
    "            M = cv.getRotationMatrix2D((cols/2,rows/2),np.random.uniform(0.0,360.0,1),1) \n",
    "            # applies affine rotation\n",
    "            res = cv.warpAffine(res,M,(cols,rows))\n",
    "\n",
    "        # Change to gray scale if input argument num_color_channels = 1\n",
    "        if num_color_channels == 1: \n",
    "            res = cv.cvtColor(res, cv.COLOR_BGR2GRAY)# convert from bgr to gray\n",
    "            res = res[...,None] # add extra dimension with blank values to very end, needed for keras\n",
    "            \n",
    "        batch_out[count%batch_size,...] = res # put image in position in batch, never to exceed size of batch\n",
    "        \n",
    "        for k in label_map.keys():\n",
    "            if k in f: #if a category name is found in the path to the file of the image\n",
    "                labels_out[count%batch_size,:] = np_utils.to_categorical(label_map[k],num_classes) #one hot labeling\n",
    "                break   \n",
    "                \n",
    "        count += 1\n",
    "        if count == num_files:# if gone through all files, restart the counter\n",
    "            count = 0\n",
    "        if count%batch_size == 0: #if gone through enough files to make a full batch\n",
    "            if predict: # i.e., there is no label for this batch of images, so in prediction mode\n",
    "                yield batch_out.astype(np.float)/255.\n",
    "            else: # training\n",
    "                yield batch_out.astype(np.float)/255., labels_out\n",
    "            \n",
    "            \n",
    "            \n",
    "def convert_to_class(prediction,label_map):\n",
    "    predict_max = np.argmax(prediction,axis=1)#provides index of max value out of prediction classes\n",
    "    predict_label = []\n",
    "    for i in range(len(predict_max)):\n",
    "        for k,v in label_map.items():\n",
    "                if predict_max[i] == v:\n",
    "                    predict_label.append(k)\n",
    "    return predict_label    \n",
    "\n",
    "def prop_correct(predict_label,actual_label):\n",
    "    correct_class = []\n",
    "    for i in range(len(predict_label)):\n",
    "        if predict_label[i]==actual_label[i]:\n",
    "            correct_class.append(1)\n",
    "        else:\n",
    "            correct_class.append(0)\n",
    "    num_correct = sum(correct_class)\n",
    "    proportion_correct = num_correct/len(predict_label)\n",
    "    return proportion_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of files is  3682\n",
      "example file names are  ['/home/guest_1/Image_classification_SPCS/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other/Lingulodinium_polyedra/SPCP2-1559624849-002615-000-448-2152-120-128.jpg', '/home/guest_1/Image_classification_SPCS/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other/Lingulodinium_polyedra/SPCP2-1558393872-051894-001-56-2480-112-120.jpg', '/home/guest_1/Image_classification_SPCS/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other/Lingulodinium_polyedra/SPCP2-1559582230-045258-001-128-484-128-112.jpg', '/home/guest_1/Image_classification_SPCS/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other/Lingulodinium_polyedra/SPCP2-1559583581-055949-000-524-2428-120-128.jpg']\n",
      "Over all images - minimum height: 24, minimum width: 32, maximum height: 312, maximum width:448\n",
      "{'Ciliate': 0, 'Lingulodinium_polyedra': 1, 'Other': 2}\n",
      "length of trainig data is  2210\n",
      "length of trainig data is  738\n",
      "length of trainig data is  734\n",
      "train labels length is  2210\n",
      "validation labels length is 738\n",
      "test labels length is 734\n"
     ]
    }
   ],
   "source": [
    "# Get full paths to all classification data\n",
    "# Data is assumed to reside under the directory \"root_dir\", and data for each class is assumed to reside in a separate subfolder\n",
    "\n",
    "# TO DO: define in the variable root_dir the directory path to where the folders with the images are located\n",
    "root_dir = '/home/guest_1/Image_classification_SPCS/Scripps_plankton_camera_system_images/Labeled_ciliates_and_other'\n",
    "\n",
    "\n",
    "# TO DO: add in any additional image types in path above that are not already listed in the img_types variable below\n",
    "img_types=['.jpg', '.tiff', '.tif', '.png', '.jpeg']\n",
    "\n",
    "files = get_image_files(root_dir, img_types)\n",
    "\n",
    "print('number of files is ',len(files))\n",
    "print('example file names are ', files[0:4])\n",
    "\n",
    "# Get the dimension range of the data for informational purposes\n",
    "minh,minw,maxh,maxw = get_dimensions(files)\n",
    "print('Over all images - minimum height: {}, minimum width: {}, maximum height: {}, maximum width:{}'.format(minh,minw,maxh,maxw))\n",
    "\n",
    "# Assign numerical labels to categories - the number of categories is equal to the number of subfolders]\n",
    "label_map = make_labels(files)\n",
    "\n",
    "print(label_map)\n",
    "\n",
    "# TO DO: Using the appropriate utility function from cell 2, divide data into training, validation, and testing data\n",
    "# Variable names should be as follows:\n",
    "#train_files=training data\n",
    "#val_files= validation data\n",
    "#test_files=testing data\n",
    "#train_labels_name=training labels\n",
    "#val_labels_name=validation labels\n",
    "#test_labels_name=testing data labels\n",
    "\n",
    "train_files, val_files, test_files, train_labels_name, val_labels_name, test_labels_name = make_train_val_test(files,label_map)\n",
    "\n",
    "# Print length of each data set and labels array\n",
    "print('length of trainig data is ',len(train_files))\n",
    "print('length of trainig data is ',len(val_files))\n",
    "print('length of trainig data is ',len(test_files))\n",
    "\n",
    "print('train labels length is ',len(train_labels_name))\n",
    "print('validation labels length is', len(val_labels_name))\n",
    "print('test labels length is', len(test_labels_name))    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine tuning\n",
    "The code below was modified from https://keras.io/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes and must be adapted for use with xception instead of InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4a. Creating base pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base pre-trained model\n",
    "# TO DO: Think about if you should include the top layers (the layers used for classification in the ORIGINAL model). \n",
    "# If you should, type in include_top = True, otherwise, use include_top = False\n",
    "\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully-connected layer\n",
    "x = Dense(4, activation='relu')(x)\n",
    "\n",
    "# Add logistic layer -- let's say we have x classes--determined by len(label_map)\n",
    "predictions = Dense(len(label_map), activation='softmax')(x)\n",
    "\n",
    "# Below is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile the model (should be done AFTER setting layers to non-trainable)\n",
    "model.compile(optimizer='adam', loss = 'categorical_crossentropy',metrics= ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4b. Training existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "69/69 [==============================] - 26s 378ms/step - loss: 0.8501 - acc: 0.6567 - val_loss: 0.8723 - val_acc: 0.6671\n",
      "Epoch 2/1000\n",
      "69/69 [==============================] - 19s 273ms/step - loss: 0.7341 - acc: 0.6739 - val_loss: 0.7938 - val_acc: 0.6586\n",
      "Epoch 3/1000\n",
      "69/69 [==============================] - 19s 277ms/step - loss: 0.6616 - acc: 0.7156 - val_loss: 0.7061 - val_acc: 0.6912\n",
      "Epoch 4/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.6043 - acc: 0.7591 - val_loss: 0.6750 - val_acc: 0.6898\n",
      "Epoch 5/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.5614 - acc: 0.7826 - val_loss: 0.7115 - val_acc: 0.6870\n",
      "Epoch 6/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.5485 - acc: 0.7909 - val_loss: 0.6415 - val_acc: 0.7210\n",
      "Epoch 7/1000\n",
      "69/69 [==============================] - 19s 273ms/step - loss: 0.5101 - acc: 0.8189 - val_loss: 0.6717 - val_acc: 0.7167\n",
      "Epoch 8/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4952 - acc: 0.8225 - val_loss: 0.6234 - val_acc: 0.7181\n",
      "Epoch 9/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4719 - acc: 0.8262 - val_loss: 0.6576 - val_acc: 0.7195\n",
      "Epoch 10/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4588 - acc: 0.8320 - val_loss: 0.6583 - val_acc: 0.7238\n",
      "Epoch 11/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4310 - acc: 0.8455 - val_loss: 0.6443 - val_acc: 0.7295\n",
      "Epoch 12/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4764 - acc: 0.8362 - val_loss: 0.6820 - val_acc: 0.7280\n",
      "Epoch 13/1000\n",
      "69/69 [==============================] - 19s 273ms/step - loss: 0.5370 - acc: 0.8176 - val_loss: 0.6669 - val_acc: 0.7238\n",
      "Epoch 14/1000\n",
      "69/69 [==============================] - 19s 273ms/step - loss: 0.4440 - acc: 0.8429 - val_loss: 0.6107 - val_acc: 0.7705\n",
      "Epoch 15/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4288 - acc: 0.8541 - val_loss: 0.6481 - val_acc: 0.7224\n",
      "Epoch 16/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4098 - acc: 0.8664 - val_loss: 0.6174 - val_acc: 0.7592\n",
      "Epoch 17/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4638 - acc: 0.8380 - val_loss: 0.7071 - val_acc: 0.7507\n",
      "Epoch 18/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.3967 - acc: 0.8673 - val_loss: 0.5569 - val_acc: 0.7790\n",
      "Epoch 19/1000\n",
      "69/69 [==============================] - 19s 280ms/step - loss: 0.3758 - acc: 0.8822 - val_loss: 0.6933 - val_acc: 0.7280\n",
      "Epoch 20/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3992 - acc: 0.8605 - val_loss: 0.6150 - val_acc: 0.7521\n",
      "Epoch 21/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3969 - acc: 0.8587 - val_loss: 0.6324 - val_acc: 0.7748\n",
      "Epoch 22/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.4181 - acc: 0.8539 - val_loss: 0.5867 - val_acc: 0.7691\n",
      "Epoch 23/1000\n",
      "69/69 [==============================] - 19s 270ms/step - loss: 0.4083 - acc: 0.8565 - val_loss: 0.6062 - val_acc: 0.7734\n",
      "Epoch 24/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.4088 - acc: 0.8579 - val_loss: 0.6208 - val_acc: 0.7550\n",
      "Epoch 25/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.4051 - acc: 0.8642 - val_loss: 0.5870 - val_acc: 0.7649\n",
      "Epoch 26/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.4002 - acc: 0.8619 - val_loss: 0.6319 - val_acc: 0.7550\n",
      "Epoch 27/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3819 - acc: 0.8506 - val_loss: 0.5726 - val_acc: 0.7762\n",
      "Epoch 28/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3631 - acc: 0.8615 - val_loss: 0.6070 - val_acc: 0.7705\n",
      "Epoch 29/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3934 - acc: 0.8701 - val_loss: 0.5707 - val_acc: 0.8059\n",
      "Epoch 30/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3528 - acc: 0.8701 - val_loss: 0.6338 - val_acc: 0.7564\n",
      "Epoch 31/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3902 - acc: 0.8565 - val_loss: 0.5341 - val_acc: 0.8017\n",
      "Epoch 32/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3896 - acc: 0.8714 - val_loss: 0.6134 - val_acc: 0.7776\n",
      "Epoch 33/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.4218 - acc: 0.8524 - val_loss: 0.5674 - val_acc: 0.8130\n",
      "Epoch 34/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3754 - acc: 0.8628 - val_loss: 0.5850 - val_acc: 0.7819\n",
      "Epoch 35/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3694 - acc: 0.8683 - val_loss: 0.5700 - val_acc: 0.7776\n",
      "Epoch 36/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3714 - acc: 0.8677 - val_loss: 0.5734 - val_acc: 0.7847\n",
      "Epoch 37/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3523 - acc: 0.8750 - val_loss: 0.5547 - val_acc: 0.7790\n",
      "Epoch 38/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3698 - acc: 0.8701 - val_loss: 0.6004 - val_acc: 0.7606\n",
      "Epoch 39/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3528 - acc: 0.8674 - val_loss: 0.4751 - val_acc: 0.8074\n",
      "Epoch 40/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3813 - acc: 0.8751 - val_loss: 0.6355 - val_acc: 0.7776\n",
      "Epoch 41/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3311 - acc: 0.8872 - val_loss: 0.6374 - val_acc: 0.7875\n",
      "Epoch 42/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3725 - acc: 0.8529 - val_loss: 0.5450 - val_acc: 0.7663\n",
      "Epoch 43/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3480 - acc: 0.8737 - val_loss: 0.5442 - val_acc: 0.8031\n",
      "Epoch 44/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3832 - acc: 0.8660 - val_loss: 0.5648 - val_acc: 0.7847\n",
      "Epoch 45/1000\n",
      "69/69 [==============================] - 19s 270ms/step - loss: 0.3561 - acc: 0.8763 - val_loss: 0.6619 - val_acc: 0.7592\n",
      "Epoch 46/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3228 - acc: 0.8863 - val_loss: 0.5428 - val_acc: 0.7960\n",
      "Epoch 47/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3655 - acc: 0.8664 - val_loss: 0.5635 - val_acc: 0.7875\n",
      "Epoch 48/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3524 - acc: 0.8777 - val_loss: 0.5893 - val_acc: 0.7762\n",
      "Epoch 49/1000\n",
      "69/69 [==============================] - 19s 274ms/step - loss: 0.3365 - acc: 0.8718 - val_loss: 0.5487 - val_acc: 0.7826\n",
      "Epoch 50/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3778 - acc: 0.8687 - val_loss: 0.5852 - val_acc: 0.7932\n",
      "Epoch 51/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3523 - acc: 0.8695 - val_loss: 0.6130 - val_acc: 0.7805\n",
      "Epoch 52/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3364 - acc: 0.8781 - val_loss: 0.5851 - val_acc: 0.7776\n",
      "Epoch 53/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3822 - acc: 0.8539 - val_loss: 0.6062 - val_acc: 0.7705\n",
      "Epoch 54/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.4513 - acc: 0.8340 - val_loss: 0.5485 - val_acc: 0.7833\n",
      "Epoch 55/1000\n",
      "69/69 [==============================] - 19s 272ms/step - loss: 0.3164 - acc: 0.8877 - val_loss: 0.5845 - val_acc: 0.7918\n",
      "Epoch 56/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3435 - acc: 0.8724 - val_loss: 0.6046 - val_acc: 0.7790\n",
      "Epoch 57/1000\n",
      "69/69 [==============================] - 19s 270ms/step - loss: 0.3321 - acc: 0.8795 - val_loss: 0.6220 - val_acc: 0.7776\n",
      "Epoch 58/1000\n",
      "69/69 [==============================] - 19s 270ms/step - loss: 0.3175 - acc: 0.8827 - val_loss: 0.5911 - val_acc: 0.7847\n",
      "Epoch 59/1000\n",
      "69/69 [==============================] - 19s 271ms/step - loss: 0.3231 - acc: 0.8745 - val_loss: 0.6281 - val_acc: 0.7705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff8ace5c2b0>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO DO: try the following batch sizes, one at a time: 16, 32, 64, recording accuracy for all\n",
    "BS = 32\n",
    "\n",
    "# Initializing other parameters\n",
    "EPOCHS = 1000\n",
    "im_wid = 150\n",
    "im_height = 150 \n",
    "\n",
    "# Construct the training image generator for data augmentation\n",
    "data_gen = ImageDataGenerator(featurewise_center = False, samplewise_center = False,\n",
    "                             featurewise_std_normalization = False, samplewise_std_normalization=False,\n",
    "                             rotation_range = 360, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                             zoom_range = 0.5, fill_mode = 'constant',cval=0,horizontal_flip = True,\n",
    "                             vertical_flip = True, rescale = None)\n",
    "\n",
    "# Get array of training and validaiton images \n",
    "train_gen = get_batches(train_files, label_map, batch_size = len(train_files),resize_size=[im_height,im_wid],\n",
    "                       num_color_channels=3)\n",
    "val_gen = get_batches(val_files,label_map,batch_size = len(val_files),resize_size=[im_height,im_wid],\n",
    "                     num_color_channels = 3)\n",
    "\n",
    "\n",
    "train_data, train_labels_oh = next(train_gen) #one-hot encoded data\n",
    "val_data, val_labels_oh = next(val_gen)\n",
    "\n",
    "# Train the network\n",
    "ES = EarlyStopping(monitor='val_loss',patience=20,verbose=0)# callback to stop if validation loss has not improved in 20 iterations\n",
    "model.fit_generator(data_gen.flow(train_data,train_labels_oh, batch_size = BS),\n",
    "                   steps_per_epoch=len(train_files)//BS,epochs = EPOCHS,\n",
    "                   validation_data=data_gen.flow(val_data,val_labels_oh,batch_size=BS),\n",
    "                   validation_steps = len(val_files)//BS,\n",
    "                   callbacks=[ES])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Predict using test data\n",
    "predict_gen = get_batches(test_files,label_map,batch_size=1,resize_size=[im_height,im_wid],\n",
    "                          num_color_channels=3, predict = True,do_shuffle=False)\n",
    "prediction = model.predict_generator(predict_gen,steps = len(test_files))\n",
    "\n",
    "# TO DO: Use the appropriate utility function from cell 2 to convert predictions (saved in the variable prediction\n",
    "    # to a classification category\n",
    "    # Save the output in the variable predict_class\n",
    "\n",
    "predict_class = convert_to_class(prediction,label_map)\n",
    "\n",
    "# TO DO: Determine the proportion of classifications that were classified correctly using the appropriate utility\n",
    "# function from cell 2. Save that proportion in the variable proportion_correct\n",
    " \n",
    "proportion_correct = num_correct/len(predict_label)\n",
    "    \n",
    "# Printing proportion correct  \n",
    "\n",
    "print(proportion_correct)\n",
    "\n",
    "# TO DO: record the variable settings (e.g., CNN architecture, batch size, epochs, optimizers, and proportion correct) \n",
    "    # in some document (e.g., excel spreadsheet, git, etc.)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model and compiling first layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#base_model = Xception(include_top=False, weights='imagenet', input_tensor=None, input_shape=(880,920,3), pooling=None)\n",
    "base_modelX = Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "# Adding global spatial average pooling layer\n",
    "xX = base_modelX.output\n",
    "xX = GlobalAveragePooling2D()(xX)\n",
    "\n",
    "# Adding in fully-connected layer\n",
    "xX = Dense(4, activation='relu')(xX)\n",
    "\n",
    "# Logistic layer for number of classes\n",
    "## [Add in flexible number of classes]\n",
    "predictions = Dense(len(label_map),activation='softmax')(xX)\n",
    "\n",
    "# Model that will be trained\n",
    "modelX = Model(inputs = base_modelX.input, outputs = predictions)\n",
    "\n",
    "# Training only top layers\n",
    "for layer in base_modelX.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# Compile model\n",
    "## [Choose different optimizers]\n",
    "modelX.compile(optimizer='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-d4c32f204ff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m val_gen = get_batches(val_files, label_map,batch_size = len(val_files),\n\u001b[1;32m     19\u001b[0m                      resize_size = [im_height,im_wid],num_color_channels=3)\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_labels_oh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_gen' is not defined"
     ]
    }
   ],
   "source": [
    "## [Vary batch size, maybe epochs]\n",
    "EPOCHS = 1000\n",
    "BS = 32\n",
    "\n",
    "im_height = 150\n",
    "im_wid = 150\n",
    "\n",
    "# Creating training image generator for data augmentation\n",
    "data_gen = ImageDataGenerator(featurewise_center = False, samplewise_center = False, \n",
    "                             featurewise_std_normalization = False, samplewise_std_normalization = False,\n",
    "                             rotation_range = 360, width_shift_range = 0.2, height_shift_range = 0.2, \n",
    "                             zoom_range = 0.5, fill_mode = 'constant',cval = 0, horizontal_flip = True,\n",
    "                             vertical_flip = True, rescale = None)\n",
    "\n",
    "# Getting training and validation generators\n",
    "train_gen = get_batches(train_files, label_map, batch_size = len(train_files),\n",
    "                       resize_size = [im_height,im_wid],num_color_channels=3)\n",
    "val_gen = get_batches(val_files, label_map,batch_size = len(val_files),\n",
    "                     resize_size = [im_height,im_wid],num_color_channels=3)\n",
    "train_data,train_labels_oh = next(batch_gen)\n",
    "val_data,val_labels_oh = next(val_gen)\n",
    "\n",
    "# Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
